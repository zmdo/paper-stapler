**Referenced from** : https://arxiv.org/abs/1806.05236 <br>
[Submitted on 13 Jun 2018 (v1), last revised 11 May 2019 (this version, v7)]

## **Manifold Mixup: Better Representations by Interpolating Hidden States**
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio

> Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.

Comments:	To appear in ICML 2019 <br>
Subjects:	**Machine Learning \([stat.ML](https://arxiv.org/abs/1806.05236?context=stat.ML)\)**; Artificial Intelligence \([cs.AI](https://arxiv.org/abs/1806.05236?context=cs.AI)\); Machine Learning \([cs.LG](https://arxiv.org/abs/1806.05236?context=cs.LG)\); Neural and Evolutionary Computing \([cs.NE](https://arxiv.org/abs/1806.05236?context=cs.NE)\) <br>
Cite as:	[arXiv:1806.05236](https://arxiv.org/abs/1806.05236) **\[[stat.ML](https://arxiv.org/abs/1806.05236?context=stat.ML)\]** <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 	(or [arXiv:1806.05236v7](https://arxiv.org/abs/1806.05236v7) **\[stat.ML\]** for this version)
